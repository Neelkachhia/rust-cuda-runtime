# Rust-Managed CUDA Runtime (Vector Addition)

A minimal **Rust + C++ + CUDA** project that demonstrates **safe GPU memory management**, **Rustâ€“CUDA FFI integration**, and **end-to-end GPU computation** using NVIDIA CUDA.

This project is intentionally small but deep, focusing on **systems-level correctness, ownership, and performance** rather than large frameworks.

---

## ğŸš€ Key Highlights

- **Rust-managed GPU memory** using RAII (`DeviceBuffer<T>`)
- **CUDA kernel execution** via a minimal C++/CUDA layer
- **Safe Rust API with isolated `unsafe` FFI boundary**
- Explicit **host â†” device memory transfers**
- Fully tested **end-to-end GPU computation**

---

## ğŸ§  Motivation

CUDA programming in C++ is powerful but error-prone:
- GPU memory leaks
- Use-after-free bugs
- Manual lifetime management

Rust provides:
- Ownership and lifetimes
- Deterministic cleanup (`Drop`)
- Compile-time safety

This project combines **Rust for safety and orchestration** with **CUDA/C++ for raw GPU execution**, following patterns used in real GPU runtimes.

---

## ğŸ—ï¸ Architecture Overview

Rust (host, safe API)
â”œâ”€â”€ DeviceBuffer<T> // owns GPU memory
â”œâ”€â”€ Host â†” Device copies
â”œâ”€â”€ Kernel launch (FFI)
â”‚
â””â”€â”€ FFI boundary (unsafe)
â†“
C++ / CUDA
â”œâ”€â”€ cudaMalloc / cudaFree
â”œâ”€â”€ cudaMemcpy
â”œâ”€â”€ Kernel launcher
â””â”€â”€ CUDA kernel
â†“
NVIDIA GPU

## ğŸ“‚ Project Structure

rust_cuda_runtime/
â”œâ”€â”€ src/
â”‚ â””â”€â”€ lib.rs # Rust API + DeviceBuffer
â”œâ”€â”€ cuda/
â”‚ â”œâ”€â”€ vector_add.cu # CUDA kernel + C interface
â”‚ â”œâ”€â”€ vector_add.h # C header for FFI
â”‚ â””â”€â”€ libvector_add.a # CUDA static library
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ vector_add.rs # End-to-end GPU test
â”œâ”€â”€ build.rs # Cargo build script (linking)
â””â”€â”€ Cargo.toml


## ğŸ”‘ Core Components

### 1ï¸âƒ£ `DeviceBuffer<T>` (Rust)

- Wraps `cudaMalloc` / `cudaFree`
- Enforces **single ownership**
- Automatically frees GPU memory via `Drop`
- Prevents leaks and misuse

```rust
let d_buf = DeviceBuffer::<f32>::new(1024);
// GPU memory freed automatically when dropped

2ï¸âƒ£ CUDA Kernel (C++)

__global__ void vector_add(const float* a,
                           const float* b,
                           float* c,
                           int n);

Executed on GPU

Launched via a C-compatible wrapper

Synchronized explicitly

3ï¸âƒ£ Rust â†” CUDA FFI

extern "C" interface

All unsafety isolated at the boundary

Rust API remains safe and ergonomic

ğŸ§ª Testing

An end-to-end test validates:

Host â†’ Device copy

Kernel execution

Device â†’ Host copy

Correct numerical results

Run tests:
cargo test

Expected output:
test test_vector_add ... ok

ğŸ› ï¸ Build & Requirements
Requirements

Linux / WSL2

NVIDIA GPU

CUDA Toolkit (tested with CUDA 12.9)

Rust (stable)

Build

cargo build


ğŸ§© Why This Project Is Small (On Purpose)

This project avoids large frameworks to:

Make every line explainable

Focus on GPU systems fundamentals

Demonstrate engineering judgment

The goal is depth, not breadth.

ğŸ“Œ What This Demonstrates

GPU memory lifecycle management

Host vs device execution model

Rust ownership applied to GPUs

CUDA runtime integration

Build systems & linker knowledge